# Disaster-tweets
NLP stands for Natural Language Processing, which is a subfield of computer science and artificial intelligence that is used to help computers better understand and interact with human language. It involves developing algorithms and computational models to process, understand, and generate natural language data, such as text .

In this context, we aim to classify tweets related to disasters as either real or fake. During disasters, social media platforms such as Twitter are often used to spread information and help coordinate relief efforts. However, not all tweets related to disasters are genuine, and there is a need to filter out fake information. We will use machine learning algorithms to develop a model that can accurately classify tweets related to disasters as real or fake.

We used a dataset of tweets related to disasters, labeled as either real or fake. We preprocessed the data by cleaning and transforming it, and then used a machine learning algorithm such as Logistic Regression to train and test our model. We used techniques such as feature extraction to improve the accuracy of our predictions.

Finally, We evaluated the performance of our model using metrics such as accuracy, precision, recall, and F1-score. 

By accurately classifying disaster tweets as real or fake, we can help prevent the spread of false information during disasters and ensure that accurate information reaches the people who need it. This project demonstrates the use of machine learning in solving real-world problems, and can be applied to other industries and domains that require accurate classification of text data.

**Tools:** <br /> Programming language: Python <br />
            Data preprocessing and analysis libraries: pandas, NumPy, scikit-learn and keras <br />
            Natural language processing libraries: Tokenizer, CountVectorizer and TfidfVectorizer <br />
            Machine learning algorithms: Logistic Regression
