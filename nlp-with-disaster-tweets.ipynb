{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-07-06T21:53:16.278328Z","iopub.status.busy":"2021-07-06T21:53:16.277887Z","iopub.status.idle":"2021-07-06T21:53:16.307880Z","shell.execute_reply":"2021-07-06T21:53:16.306227Z","shell.execute_reply.started":"2021-07-06T21:53:16.278291Z"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1633531826360,"user":{"displayName":"ines souissi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10446406516205922358"},"user_tz":-60},"id":"rgeNtLmm1Taq","trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","\n","import numpy as np \n","import pandas as pd \n","from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n","import os\n","import warnings\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from IPython.display import display\n","from pandas.api.types import CategoricalDtype\n","\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","from sklearn.feature_selection import mutual_info_regression\n","from sklearn.model_selection import KFold, cross_val_score\n","from xgboost import XGBRegressor\n","import os\n","\n","import sklearn\n","import keras\n","import nltk\n","import pandas as pd\n","import numpy as np\n","import re\n","import codecs\n"]},{"cell_type":"markdown","metadata":{"id":"IJQFfc4-3078"},"source":["# Nouvelle section"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:16.410782Z","iopub.status.busy":"2021-07-06T21:53:16.410362Z","iopub.status.idle":"2021-07-06T21:53:16.481219Z","shell.execute_reply":"2021-07-06T21:53:16.479713Z","shell.execute_reply.started":"2021-07-06T21:53:16.410739Z"},"id":"mPvujWNU1Tat","trusted":true},"outputs":[],"source":["\n","df_train = pd.read_csv(\"/content/train.csv\")\n","df_test = pd.read_csv(\"/content/test.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:16.528430Z","iopub.status.busy":"2021-07-06T21:53:16.527785Z","iopub.status.idle":"2021-07-06T21:53:16.550917Z","shell.execute_reply":"2021-07-06T21:53:16.549311Z","shell.execute_reply.started":"2021-07-06T21:53:16.528379Z"},"id":"ieiZ6Ms21Tau","trusted":true},"outputs":[],"source":["df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:16.767273Z","iopub.status.busy":"2021-07-06T21:53:16.766792Z","iopub.status.idle":"2021-07-06T21:53:16.885403Z","shell.execute_reply":"2021-07-06T21:53:16.883958Z","shell.execute_reply.started":"2021-07-06T21:53:16.767235Z"},"id":"AtuUq6f71Tav","trusted":true},"outputs":[],"source":["input_file = codecs.open(\"/content/train.csv\", \"r\",encoding='utf-8', errors='replace')\n","output_file = open(\"train_output.csv\", \"w\")\n","\n","def sanitize_characters(raw, clean):    \n","    for line in input_file:\n","        out = line\n","        output_file.write(line)\n","sanitize_characters(input_file, output_file)\n","\n","questions = pd.read_csv(\"/content/train_output.csv\")\n","questions.columns=['id','keyword', 'location', 'text', 'targte']\n","questions.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:16.892093Z","iopub.status.busy":"2021-07-06T21:53:16.891613Z","iopub.status.idle":"2021-07-06T21:53:17.112955Z","shell.execute_reply":"2021-07-06T21:53:17.111752Z","shell.execute_reply.started":"2021-07-06T21:53:16.892049Z"},"id":"FoqscFQA1Taw","trusted":true},"outputs":[],"source":["\n","def standardize_text(df, text_field):\n","    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n","    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n","    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n","    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n","    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n","    df[text_field] = df[text_field].str.lower()\n","    return df\n","\n","df_train = standardize_text(df_train, \"text\")\n","df_train.to_csv(\"clean_data.csv\")\n","df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:17.114947Z","iopub.status.busy":"2021-07-06T21:53:17.114652Z","iopub.status.idle":"2021-07-06T21:53:17.133132Z","shell.execute_reply":"2021-07-06T21:53:17.131535Z","shell.execute_reply.started":"2021-07-06T21:53:17.114918Z"},"id":"8kzqsz4y1Taw","trusted":true},"outputs":[],"source":["df_train.groupby(\"target\").count()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:17.135766Z","iopub.status.busy":"2021-07-06T21:53:17.135410Z","iopub.status.idle":"2021-07-06T21:53:17.230662Z","shell.execute_reply":"2021-07-06T21:53:17.229122Z","shell.execute_reply.started":"2021-07-06T21:53:17.135732Z"},"id":"41q02yv71Tax","trusted":true},"outputs":[],"source":["from nltk.tokenize import RegexpTokenizer\n","\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","df_train[\"tokens\"] = df_train[\"text\"].apply(tokenizer.tokenize)\n","df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:17.233758Z","iopub.status.busy":"2021-07-06T21:53:17.233224Z","iopub.status.idle":"2021-07-06T21:53:17.271317Z","shell.execute_reply":"2021-07-06T21:53:17.269774Z","shell.execute_reply.started":"2021-07-06T21:53:17.233703Z"},"id":"NmKhbNKx1Tax","trusted":true},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","all_words = [word for tokens in df_train[\"tokens\"] for word in tokens]\n","sentence_lengths = [len(tokens) for tokens in df_train[\"tokens\"]]\n","VOCAB = sorted(list(set(all_words)))\n","print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n","print(\"Max sentence length is %s\" % max(sentence_lengths))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:17.332223Z","iopub.status.busy":"2021-07-06T21:53:17.331807Z","iopub.status.idle":"2021-07-06T21:53:17.586584Z","shell.execute_reply":"2021-07-06T21:53:17.585414Z","shell.execute_reply.started":"2021-07-06T21:53:17.332186Z"},"id":"rAw5oPsL1Tay","trusted":true},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(10, 10)) \n","plt.xlabel('Sentence length')\n","plt.ylabel('Number of sentences')\n","plt.hist(sentence_lengths)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:17.588479Z","iopub.status.busy":"2021-07-06T21:53:17.588182Z","iopub.status.idle":"2021-07-06T21:53:17.854421Z","shell.execute_reply":"2021-07-06T21:53:17.852959Z","shell.execute_reply.started":"2021-07-06T21:53:17.588450Z"},"id":"pCmkcb9F1Tay","trusted":true},"outputs":[],"source":["\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","def cv(data):\n","    count_vectorizer = CountVectorizer()\n","\n","    emb = count_vectorizer.fit_transform(data)\n","\n","    return emb, count_vectorizer\n","\n","list_corpus = df_train[\"text\"].tolist()\n","list_labels = df_train[\"target\"].tolist()\n","\n","X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n","                                                                                random_state=40)\n","\n","X_train_counts, count_vectorizer = cv(X_train)\n","X_test_counts = count_vectorizer.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:17.856843Z","iopub.status.busy":"2021-07-06T21:53:17.856517Z","iopub.status.idle":"2021-07-06T21:53:18.417140Z","shell.execute_reply":"2021-07-06T21:53:18.415764Z","shell.execute_reply.started":"2021-07-06T21:53:17.856808Z"},"id":"H5NgInwN1Tay","trusted":true},"outputs":[],"source":["\n","from sklearn.decomposition import PCA, TruncatedSVD\n","import matplotlib\n","import matplotlib.patches as mpatches\n","\n","\n","def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n","        lsa = TruncatedSVD(n_components=2)\n","        lsa.fit(test_data)\n","        lsa_scores = lsa.transform(test_data)\n","        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n","        color_column = [color_mapper[label] for label in test_labels]\n","        colors = ['orange','blue','blue']\n","        if plot:\n","            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n","            red_patch = mpatches.Patch(color='orange', label='Irrelevant')\n","            green_patch = mpatches.Patch(color='blue', label='Disaster')\n","            plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n","\n","\n","fig = plt.figure(figsize=(16, 16))          \n","plot_LSA(X_train_counts, y_train)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:18.420080Z","iopub.status.busy":"2021-07-06T21:53:18.419522Z","iopub.status.idle":"2021-07-06T21:53:21.051107Z","shell.execute_reply":"2021-07-06T21:53:21.049677Z","shell.execute_reply.started":"2021-07-06T21:53:18.420036Z"},"id":"b1ybjzUI1Taz","trusted":true},"outputs":[],"source":["\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n","                         multi_class='multinomial', n_jobs=-1, random_state=40)\n","clf.fit(X_train_counts, y_train)\n","\n","y_predicted_counts = clf.predict(X_test_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:21.053707Z","iopub.status.busy":"2021-07-06T21:53:21.053083Z","iopub.status.idle":"2021-07-06T21:53:21.083021Z","shell.execute_reply":"2021-07-06T21:53:21.081444Z","shell.execute_reply.started":"2021-07-06T21:53:21.053658Z"},"id":"E7itFML91Ta0","trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n","\n","def get_metrics(y_test, y_predicted):  \n","    precision = precision_score(y_test, y_predicted, pos_label=None,\n","                                    average='weighted')             \n","\n","    recall = recall_score(y_test, y_predicted, pos_label=None,\n","                              average='weighted')\n"," \n","    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n","    \n","\n","    accuracy = accuracy_score(y_test, y_predicted)\n","    return accuracy, precision, recall, f1\n","\n","accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n","print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:21.085520Z","iopub.status.busy":"2021-07-06T21:53:21.085141Z","iopub.status.idle":"2021-07-06T21:53:21.098426Z","shell.execute_reply":"2021-07-06T21:53:21.097093Z","shell.execute_reply.started":"2021-07-06T21:53:21.085484Z"},"id":"o5vNoUIu1Ta0","trusted":true},"outputs":[],"source":["\n","import numpy as np\n","import itertools\n","from sklearn.metrics import confusion_matrix\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.winter):\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title, fontsize=30)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, fontsize=20)\n","    plt.yticks(tick_marks, classes, fontsize=20)\n","    \n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n","                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n","    \n","    plt.tight_layout()\n","    plt.ylabel('True label', fontsize=30)\n","    plt.xlabel('Predicted label', fontsize=30)\n","\n","    return plt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:21.101907Z","iopub.status.busy":"2021-07-06T21:53:21.101578Z","iopub.status.idle":"2021-07-06T21:53:21.431332Z","shell.execute_reply":"2021-07-06T21:53:21.430306Z","shell.execute_reply.started":"2021-07-06T21:53:21.101874Z"},"id":"r6rX-TEB1Ta1","trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_test, y_predicted_counts)\n","fig = plt.figure(figsize=(10, 10))\n","plot = plot_confusion_matrix(cm, classes=['Irrelevant','Disaster'], normalize=False, title='Confusion matrix')\n","plt.show()\n","print(cm)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:21.433726Z","iopub.status.busy":"2021-07-06T21:53:21.433319Z","iopub.status.idle":"2021-07-06T21:53:21.468693Z","shell.execute_reply":"2021-07-06T21:53:21.467134Z","shell.execute_reply.started":"2021-07-06T21:53:21.433685Z"},"id":"OLjxVz6m1Ta1","trusted":true},"outputs":[],"source":["\n","def get_most_important_features(vectorizer, model, n=5):\n","    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n","    \n","    classes ={}\n","    for class_index in range(model.coef_.shape[0]):\n","        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n","        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n","        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n","        bottom = sorted_coeff[-n:]\n","        classes[class_index] = {\n","            'tops':tops,\n","            'bottom':bottom\n","        }\n","    return classes\n","\n","importance = get_most_important_features(count_vectorizer, clf, 10) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:21.470550Z","iopub.status.busy":"2021-07-06T21:53:21.470232Z","iopub.status.idle":"2021-07-06T21:53:21.845500Z","shell.execute_reply":"2021-07-06T21:53:21.844174Z","shell.execute_reply.started":"2021-07-06T21:53:21.470518Z"},"id":"uVZb3oZx1Ta1","trusted":true},"outputs":[],"source":["#To see importance of words \n","\n","def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n","    y_pos = np.arange(len(top_words))\n","    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n","    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n","    \n","    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n","    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n","    \n","    top_words = [a[0] for a in top_pairs]\n","    top_scores = [a[1] for a in top_pairs]\n","    \n","    bottom_words = [a[0] for a in bottom_pairs]\n","    bottom_scores = [a[1] for a in bottom_pairs]\n","    \n","    fig = plt.figure(figsize=(10, 10))  \n","\n","    plt.subplot(121)\n","    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n","    plt.title('Irrelevant', fontsize=20)\n","    plt.yticks(y_pos, bottom_words, fontsize=14)\n","    plt.suptitle('Key words', fontsize=16)\n","    plt.xlabel('Importance', fontsize=20)\n","    \n","    plt.subplot(122)\n","    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n","    plt.title('Disaster', fontsize=20)\n","    plt.yticks(y_pos, top_words, fontsize=14)\n","    plt.suptitle(name, fontsize=16)\n","    plt.xlabel('Importance', fontsize=20)\n","    \n","    plt.subplots_adjust(wspace=0.8)\n","    plt.show()\n","\n","top_scores = [a[0] for a in importance[0]['tops']]\n","top_words = [a[1] for a in importance[0]['tops']]\n","bottom_scores = [a[0] for a in importance[0]['bottom']]\n","bottom_words = [a[1] for a in importance[0]['bottom']]\n","\n","plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:21.847861Z","iopub.status.busy":"2021-07-06T21:53:21.847497Z","iopub.status.idle":"2021-07-06T21:53:22.123504Z","shell.execute_reply":"2021-07-06T21:53:22.122082Z","shell.execute_reply.started":"2021-07-06T21:53:21.847825Z"},"id":"1e18GNkZ1Ta2","trusted":true},"outputs":[],"source":["# discounting words that are too frequent, as they just add to the noise.\n","\n","def tfidf(data):\n","    tfidf_vectorizer = TfidfVectorizer()\n","\n","    train = tfidf_vectorizer.fit_transform(data)\n","\n","    return train, tfidf_vectorizer\n","\n","X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:22.126200Z","iopub.status.busy":"2021-07-06T21:53:22.125533Z","iopub.status.idle":"2021-07-06T21:53:22.130941Z","shell.execute_reply":"2021-07-06T21:53:22.129305Z","shell.execute_reply.started":"2021-07-06T21:53:22.126150Z"},"id":"l8IopzvN1Ta3","trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:22.133907Z","iopub.status.busy":"2021-07-06T21:53:22.133430Z","iopub.status.idle":"2021-07-06T21:53:23.226536Z","shell.execute_reply":"2021-07-06T21:53:23.224767Z","shell.execute_reply.started":"2021-07-06T21:53:22.133861Z"},"id":"4FILeUb21Ta3","trusted":true},"outputs":[],"source":["#Evaluation to see if our classifier performed well at all\n","clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n","                         multi_class='multinomial', n_jobs=-1, random_state=40)\n","clf_tfidf.fit(X_train_tfidf, y_train)\n","\n","y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:23.229158Z","iopub.status.busy":"2021-07-06T21:53:23.228534Z","iopub.status.idle":"2021-07-06T21:53:23.250236Z","shell.execute_reply":"2021-07-06T21:53:23.248928Z","shell.execute_reply.started":"2021-07-06T21:53:23.229091Z"},"id":"w_LkGSjj1Ta3","trusted":true},"outputs":[],"source":["accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf)\n","print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_tfidf, precision_tfidf, \n","                                                                       recall_tfidf, f1_tfidf))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:23.252245Z","iopub.status.busy":"2021-07-06T21:53:23.251725Z","iopub.status.idle":"2021-07-06T21:53:23.508358Z","shell.execute_reply":"2021-07-06T21:53:23.507139Z","shell.execute_reply.started":"2021-07-06T21:53:23.252202Z"},"id":"YfnrYeDo1Ta3","trusted":true},"outputs":[],"source":["#Confusion matrix\n","\n","cm2 = confusion_matrix(y_test, y_predicted_tfidf)\n","fig = plt.figure(figsize=(10, 10))\n","plot = plot_confusion_matrix(cm2, classes=['Irrelevant','Disaster'], normalize=False, title='Confusion matrix')\n","plt.show()\n","print(\"TFIDF confusion matrix\")\n","print(cm2)\n","print(\"BoW confusion matrix\")\n","print(cm)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:23.510371Z","iopub.status.busy":"2021-07-06T21:53:23.510032Z","iopub.status.idle":"2021-07-06T21:53:23.898480Z","shell.execute_reply":"2021-07-06T21:53:23.897329Z","shell.execute_reply.started":"2021-07-06T21:53:23.510335Z"},"id":"5gLyB1JO1Ta4","trusted":true},"outputs":[],"source":["#Looking at important coefficients for linear regression\n","\n","importance_tfidf = get_most_important_features(tfidf_vectorizer, clf_tfidf, 10)\n","\n","top_scores = [a[0] for a in importance_tfidf[0]['tops']]\n","top_words = [a[1] for a in importance_tfidf[0]['tops']]\n","bottom_scores = [a[0] for a in importance_tfidf[0]['bottom']]\n","bottom_words = [a[1] for a in importance_tfidf[0]['bottom']]\n","\n","plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-07-06T20:48:19.146594Z","iopub.status.busy":"2021-07-06T20:48:19.146185Z","iopub.status.idle":"2021-07-06T20:48:19.184056Z","shell.execute_reply":"2021-07-06T20:48:19.182026Z","shell.execute_reply.started":"2021-07-06T20:48:19.14655Z"},"id":"FL2qYMmu1Ta4"},"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:53:23.900806Z","iopub.status.busy":"2021-07-06T21:53:23.900164Z","iopub.status.idle":"2021-07-06T21:54:21.309637Z","shell.execute_reply":"2021-07-06T21:54:21.308699Z","shell.execute_reply.started":"2021-07-06T21:53:23.900753Z"},"id":"6u68LbEg1Ta4","trusted":true},"outputs":[],"source":["import gzip\n","\n","import gensim\n","import gensim.downloader as api\n","from gensim.models import KeyedVectors\n","\n","\n","word2vec_path = '/content/GoogleNews-vectors-negative300.bin.zip'\n","\n","word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:54:21.311381Z","iopub.status.busy":"2021-07-06T21:54:21.310896Z","iopub.status.idle":"2021-07-06T21:54:21.320679Z","shell.execute_reply":"2021-07-06T21:54:21.319470Z","shell.execute_reply.started":"2021-07-06T21:54:21.311347Z"},"id":"F4YWkvOz1Ta4","trusted":true},"outputs":[],"source":["\n","\n","def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n","    if len(tokens_list)<1:\n","        return np.zeros(k)\n","    if generate_missing:\n","        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n","    else:\n","        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n","    length = len(vectorized)\n","    summed = np.sum(vectorized, axis=0)\n","    averaged = np.divide(summed, length)\n","    return averaged\n","\n","def get_word2vec_embeddings(vectors, df_train, generate_missing=False):\n","    embeddings = df_train['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n","                                                                                generate_missing=generate_missing))\n","    return list(embeddings)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:54:21.325764Z","iopub.status.busy":"2021-07-06T21:54:21.325366Z","iopub.status.idle":"2021-07-06T21:54:21.967700Z","shell.execute_reply":"2021-07-06T21:54:21.966440Z","shell.execute_reply.started":"2021-07-06T21:54:21.325710Z"},"id":"IisWY3Wy1Ta4","trusted":true},"outputs":[],"source":["embeddings = get_word2vec_embeddings(word2vec, df_train)\n","X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels, \n","                                                                                        test_size=0.2, random_state=40)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:54:21.970148Z","iopub.status.busy":"2021-07-06T21:54:21.969731Z","iopub.status.idle":"2021-07-06T21:54:22.932243Z","shell.execute_reply":"2021-07-06T21:54:22.931342Z","shell.execute_reply.started":"2021-07-06T21:54:21.970113Z"},"id":"y5HNkUgs1Ta5","trusted":true},"outputs":[],"source":["fig = plt.figure(figsize=(16, 16))          \n","plot_LSA(embeddings, list_labels)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:54:22.933921Z","iopub.status.busy":"2021-07-06T21:54:22.933457Z","iopub.status.idle":"2021-07-06T21:54:23.778494Z","shell.execute_reply":"2021-07-06T21:54:23.777176Z","shell.execute_reply.started":"2021-07-06T21:54:22.933869Z"},"id":"QlK0wDsn1Ta5","trusted":true},"outputs":[],"source":["#Logistic Regression\n","\n","clf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n","                         multi_class='multinomial', random_state=40)\n","clf_w2v.fit(X_train_word2vec, y_train_word2vec)\n","y_predicted_word2vec = clf_w2v.predict(X_test_word2vec)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:54:23.780940Z","iopub.status.busy":"2021-07-06T21:54:23.780271Z","iopub.status.idle":"2021-07-06T21:54:23.810161Z","shell.execute_reply":"2021-07-06T21:54:23.809006Z","shell.execute_reply.started":"2021-07-06T21:54:23.780881Z"},"id":"Zns818BE1Ta5","trusted":true},"outputs":[],"source":["accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\n","print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n","                                                                       recall_word2vec, f1_word2vec))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:54:23.812746Z","iopub.status.busy":"2021-07-06T21:54:23.811998Z","iopub.status.idle":"2021-07-06T21:54:24.168002Z","shell.execute_reply":"2021-07-06T21:54:24.166461Z","shell.execute_reply.started":"2021-07-06T21:54:23.812689Z"},"id":"gCdb2U_z1Ta5","trusted":true},"outputs":[],"source":["#Confusion matrix\n","\n","\n","cm_w2v = confusion_matrix(y_test_word2vec, y_predicted_word2vec)\n","fig = plt.figure(figsize=(10, 10))\n","plot = plot_confusion_matrix(cm, classes=['Irrelevant','Disaster'], normalize=False, title='Confusion matrix')\n","plt.show()\n","print(\"Word2Vec confusion matrix\")\n","print(cm_w2v)\n","print(\"TFIDF confusion matrix\")\n","print(cm2)\n","print(\"BoW confusion matrix\")\n","print(cm)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:54:24.170061Z","iopub.status.busy":"2021-07-06T21:54:24.169669Z","iopub.status.idle":"2021-07-06T21:54:24.683121Z","shell.execute_reply":"2021-07-06T21:54:24.681835Z","shell.execute_reply.started":"2021-07-06T21:54:24.170024Z"},"id":"tRCC_7rv1Ta6","trusted":true},"outputs":[],"source":["#Lime\n","\n","from lime import lime_text\n","from sklearn.pipeline import make_pipeline\n","from lime.lime_text import LimeTextExplainer\n","\n","X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(list_corpus, list_labels, test_size=0.2, \n","                                                                                random_state=40)\n","vector_store = word2vec\n","def word2vec_pipeline(examples):\n","    global vector_store\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    tokenized_list = []\n","    for example in examples:\n","        example_tokens = tokenizer.tokenize(example)\n","        vectorized_example = get_average_word2vec(example_tokens, vector_store, generate_missing=False, k=300)\n","        tokenized_list.append(vectorized_example)\n","    return clf_w2v.predict_proba(tokenized_list)\n","\n","c = make_pipeline(count_vectorizer, clf)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:54:24.684864Z","iopub.status.busy":"2021-07-06T21:54:24.684559Z","iopub.status.idle":"2021-07-06T21:54:24.692480Z","shell.execute_reply":"2021-07-06T21:54:24.691113Z","shell.execute_reply.started":"2021-07-06T21:54:24.684833Z"},"id":"E3w8GqwT1Ta6","trusted":true},"outputs":[],"source":["def explain_one_instance(instance, class_names):\n","    explainer = LimeTextExplainer(class_names=class_names)\n","    exp = explainer.explain_instance(instance, word2vec_pipeline, num_features=6)\n","    return exp\n","\n","def visualize_one_exp(features, labels, index, class_names = [\"irrelevant\",\"relevant\", \"unknown\"]):\n","    exp = explain_one_instance(features[index], class_names = class_names)\n","    print('Index: %d' % index)\n","    print('True class: %s' % class_names[labels[index]])\n","    exp.show_in_notebook(text=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-06T21:54:24.724633Z","iopub.status.busy":"2021-07-06T21:54:24.724271Z","iopub.status.idle":"2021-07-06T21:54:50.977846Z","shell.execute_reply":"2021-07-06T21:54:50.973138Z","shell.execute_reply.started":"2021-07-06T21:54:24.724595Z"},"id":"nsCcd2Mn1Ta6","trusted":true},"outputs":[],"source":["\n","import random\n","from collections import defaultdict\n","\n","random.seed(40)\n","\n","def get_statistical_explanation(test_set, sample_size, word2vec_pipeline, label_dict):\n","    sample_sentences = random.sample(test_set, sample_size)\n","    explainer = LimeTextExplainer()\n","    \n","    labels_to_sentences = defaultdict(list)\n","    contributors = defaultdict(dict)\n","    \n","    # First, find contributing words to each class\n","    for sentence in sample_sentences:\n","        probabilities = word2vec_pipeline([sentence])\n","        curr_label = probabilities[0].argmax()\n","        labels_to_sentences[curr_label].append(sentence)\n","        exp = explainer.explain_instance(sentence, word2vec_pipeline, num_features=6, labels=[curr_label])\n","        listed_explanation = exp.as_list(label=curr_label)\n","        \n","        for word,contributing_weight in listed_explanation:\n","            if word in contributors[curr_label]:\n","                contributors[curr_label][word].append(contributing_weight)\n","            else:\n","                contributors[curr_label][word] = [contributing_weight]    \n","    \n","    # average each word's contribution to a class, and sort them by impact\n","    average_contributions = {}\n","    sorted_contributions = {}\n","    for label,lexica in contributors.items():\n","        curr_label = label\n","        curr_lexica = lexica\n","        average_contributions[curr_label] = pd.Series(index=curr_lexica.keys())\n","        for word,scores in curr_lexica.items():\n","            average_contributions[curr_label].loc[word] = np.sum(np.array(scores))/sample_size\n","        detractors = average_contributions[curr_label].sort_values()\n","        supporters = average_contributions[curr_label].sort_values(ascending=False)\n","        sorted_contributions[label_dict[curr_label]] = {\n","            'detractors':detractors,\n","             'supporters': supporters\n","        }\n","    return sorted_contributions\n","\n","label_to_text = {\n","    0: 'Irrelevant',\n","    1: 'Relevant',\n","    2: 'Unsure'\n","}\n","sorted_contributions = get_statistical_explanation(X_test_data, 100, word2vec_pipeline, label_to_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2021-07-06T21:54:50.979229Z","iopub.status.idle":"2021-07-06T21:54:50.979839Z"},"id":"X3AtNUBh1Ta7","trusted":true},"outputs":[],"source":["\n","# First index is the class (Disaster)\n","# Second index is 0 for detractors, 1 for supporters\n","# Third is how many words we sample\n","top_words = sorted_contributions['Relevant']['supporters'][:10].index.tolist()\n","top_scores = sorted_contributions['Relevant']['supporters'][:10].tolist()\n","bottom_words = sorted_contributions['Relevant']['detractors'][:10].index.tolist()\n","bottom_scores = sorted_contributions['Relevant']['detractors'][:10].tolist()\n","\n","plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"natural-language-processing-with-disaster-tweets.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}
